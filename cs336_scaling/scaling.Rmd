---
title: "Scaling law"
output: pdf_document
date: "2024-05-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(ggplot2)
library(psych)
library(readr)
```

```{r loaddf, include=FALSE}
scaling <- read_csv("../data/scaling.csv",
                    col_types = cols(d_model = col_integer(),
                                     num_layers = col_integer(),
                                     num_heads = col_integer(),
                                     batch_size = col_integer(),
                                     learning_rate = col_double()))
df = data.table(scaling)
df[,log10_train_flops:=log10(train_flops)]
df[,log10_loss:=log10(loss)]

df = df[, .SD, .SDcols = !c("train_flops", "loss")]
```

## adaptive collection of more data
Based on the hyper param trend earlier, up to 1e16 flops the model seems to favor small batch_size, large learning rate, and small d_model. We collect following additional data

```
  for d_model in [64]:
    for num_layers in [2, 16, 24]:
      for num_heads in [2, 4, 8, 16]:
        for batch_size in [128]:
          for learning_rate in [5e-4, 1e-3]:
            for train_flops in [int(1e17)]:
```

## cor plots
Checking cor plots
```{r cars}
pairs.panels(df,
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
)
```

## argmin cor
Argmin cor again.

```{r argmin}
# Function to find the row with the smallest loss for each unique log10_train_flops
get_min_loss_per_flop = function(df) {
  # Find unique values of log10_train_flops
  unique_flops = unique(df$log10_train_flops)
  
  # Initialize an empty data.table to store results
  result = data.table()

  # Iterate over each unique log10_train_flops value
  for (flop in unique_flops) {
    # Subset the data.table for the current log10_train_flops
    subset_dt = df[log10_train_flops == flop]
    
    # Find the row with the smallest loss within the subset
    min_loss_row = subset_dt[which.min(log10_loss)]
    
    # Bind the row with the smallest loss to the result data.table
    result = rbind(result, min_loss_row)
  }

  return(result)
}

# Assuming df is your data.table
argmin_df = get_min_loss_per_flop(df)
argmin_df
```
We observe the model consistently favors small batch_size, large learning rate, and small d_model.

```{r argmin cor, warning=FALSE}
pairs.panels(argmin_df,
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
)
```

## Fitting linear regression

```{r fitting losss, warning=FALSE}
library(data.table)

# Assuming argmin_df is already your data.table
# Step 1: Remove the first row where log10_train_flops is 13
argmin_df_filtered = argmin_df[log10_train_flops != 13.00000 | !seq_len(.N) == 1]

# Step 2: Fit a linear model to the filtered data
model = lm(log10_loss ~ log10_train_flops, data = argmin_df_filtered)

# Step 3: Predict log10_loss for log10_train_flops = 19 (log10 of 1e19)
predicted_loss = predict(model, newdata = data.frame(log10_train_flops = 19))

# Print the predicted value
10**predicted_loss
```


## Conclusion

We choose `num_layers=16, d_model=64, num_heads=8, batch_size=128, learning rate = 1e-3`. We predict the losos to be `2.7`.